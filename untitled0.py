# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pJvKcJyiZ-S7wr1VXN_yTD8a6yPqCHBQ
"""

!pip install PyMuPDF

!pip install sentence-transformers

!pip install faiss-cpu

!pip install streamlit

!pip install streamlit-chat

!pip install --upgrade pillow

import streamlit as st
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import pipeline
import os

# ========================
# Helper Functions (from previous steps)
# ========================
def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.getvalue(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def chunk_text(text, chunk_size=500):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[i:i+chunk_size]))
    return chunks

# Load embedding model
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

embed_model = load_embedding_model()

# Load the HuggingFace IBM Granite model
@st.cache_resource
def load_llm_model():
    return pipeline("text-generation", model="ibm-granite/granite-3.2-2b-instruct")

pipe = load_llm_model()

def search_question(question, embeddings, index, chunks, top_k=3):
    question_embedding = embed_model.encode([question], convert_to_numpy=True)
    distances, indices = index.search(question_embedding, top_k)
    relevant_chunks = [chunks[i] for i in indices[0]]
    return " ".join(relevant_chunks)

def generate_answer(question, context):
    prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
    response = pipe([{"role": "user", "content": prompt}])
    return response[0]['generated_text']


# ========================
# Streamlit Interface
# ========================
st.title("StudyMate: AI-Powered PDF Q&A")

uploaded_files = st.file_uploader("Upload PDF(s)", accept_multiple_files=True, type=['pdf'])

if uploaded_files:
    pdf_texts = []
    for f in uploaded_files:
        # Use f.getvalue() to get the bytes of the uploaded file
        pdf_texts.append(extract_text_from_pdf(f))

    pdf_chunks = []
    for text in pdf_texts:
        pdf_chunks.extend(chunk_text(text))

    if pdf_chunks:
        embeddings = embed_model.encode(pdf_chunks, convert_to_numpy=True)
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)

        question = st.text_input("Ask your question:")

        if question:
            context = search_question(question, embeddings, index, pdf_chunks)
            answer = generate_answer(question, context)
            st.write("Answer:")
            st.write(answer)
    else:
        st.warning("Could not extract text from the uploaded PDF(s).")

!pip install Pillow<12.0